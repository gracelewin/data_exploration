{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4c44c8",
   "metadata": {},
   "source": [
    "## MOSAIKS feature extraction\n",
    "\n",
    "This tutorial demonstrates the **MOSAIKS** method for extracting _feature vectors_ from satellite imagery patches for use in downstream modeling tasks. It will show:\n",
    "- How to extract 1km$^2$ patches of Sentinel 2 multispectral imagery for a list of latitude, longitude points\n",
    "- How to extract summary features from each of these imagery patches\n",
    "- How to use the summary features in a linear model of the population density at each point\n",
    "\n",
    "### Background\n",
    "\n",
    "Consider the case where you have a dataset of latitude and longitude points assosciated with some dependent variable (for example: population density, weather, housing prices, biodiversity) and, potentially, other independent variables. You would like to model the dependent variable as a function of the independent variables, but instead of including latitude and longitude directly in this model, you would like to include some high dimensional representation of what the Earth looks like at that point (that hopefully explains some of the variance in the dependent variable!). From the computer vision literature, there are various [representation learning techniques](https://en.wikipedia.org/wiki/Feature_learning) that can be used to do this, i.e. extract _features vectors_ from imagery. This notebook gives an implementation of the technique described in [Rolf et al. 2021](https://www.nature.com/articles/s41467-021-24638-z), \"A generalizable and accessible approach to machine learning with global satellite imagery\" called Multi-task Observation using Satellite Imagery & Kitchen Sinks (**MOSAIKS**). For more information about **MOSAIKS** see the [project's webpage](http://www.globalpolicy.science/mosaiks).\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "- This example uses [Sentinel-2 Level-2A data](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a). The techniques used here apply equally well to other remote-sensing datasets.\n",
    "- If you're running this on the [Planetary Computer Hub](http://planetarycomputer.microsoft.com/compute), make sure to choose the **GPU - PyTorch** profile when presented with the form to choose your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d60ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/geopandas/dask-geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab74cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "RASTERIO_BEST_PRACTICES = dict(  # See https://github.com/pangeo-data/cog-best-practices\n",
    "    CURL_CA_BUNDLE=\"/etc/ssl/certs/ca-certificates.crt\",\n",
    "    GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
    "    AWS_NO_SIGN_REQUEST=\"YES\",\n",
    "    GDAL_MAX_RAW_BLOCK_CACHE_SIZE=\"200000000\",\n",
    "    GDAL_SWATH_SIZE=\"200000000\",\n",
    "    VSI_CURL_CACHE_SIZE=\"200000000\",\n",
    ")\n",
    "os.environ.update(RASTERIO_BEST_PRACTICES)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "import rasterio.mask\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "import dask_geopandas\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=LinAlgWarning, module=\"sklearn\")\n",
    "\n",
    "import pystac_client\n",
    "import planetary_computer as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372135a",
   "metadata": {},
   "source": [
    "First we define the pytorch model that we will use to extract the features and a helper method. The **MOSAIKS** methodology describes several ways to do this and we use the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13c154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(input_img, model, device):\n",
    "    \"\"\"Helper method for running an image patch through the model.\n",
    "\n",
    "    Args:\n",
    "        input_img (np.ndarray): Image in (C x H x W) format with a dtype of uint8.\n",
    "        model (torch.nn.Module): Feature extractor network\n",
    "    \"\"\"\n",
    "    assert len(input_img.shape) == 3\n",
    "    input_img = torch.from_numpy(input_img / 255.0).float()\n",
    "    input_img = input_img.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = model(input_img.unsqueeze(0)).cpu().numpy()\n",
    "    return feats\n",
    "\n",
    "\n",
    "class RCF(nn.Module):\n",
    "    \"\"\"A model for extracting Random Convolution Features (RCF) from input imagery.\"\"\"\n",
    "\n",
    "    def __init__(self, num_features=16, kernel_size=3, num_input_channels=3):\n",
    "        super(RCF, self).__init__()\n",
    "\n",
    "        # We create `num_features / 2` filters so require `num_features` to be divisible by 2\n",
    "        assert num_features % 2 == 0\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_input_channels,\n",
    "            num_features // 2,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=1.0)\n",
    "        nn.init.constant_(self.conv1.bias, -1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1a = F.relu(self.conv1(x), inplace=True)\n",
    "        x1b = F.relu(-self.conv1(x), inplace=True)\n",
    "\n",
    "        x1a = F.adaptive_avg_pool2d(x1a, (1, 1)).squeeze()\n",
    "        x1b = F.adaptive_avg_pool2d(x1b, (1, 1)).squeeze()\n",
    "\n",
    "        if len(x1a.shape) == 1:  # case where we passed a single input\n",
    "            return torch.cat((x1a, x1b), dim=0)\n",
    "        elif len(x1a.shape) == 2:  # case where we passed a batch of > 1 inputs\n",
    "            return torch.cat((x1a, x1b), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f303c0",
   "metadata": {},
   "source": [
    "Next, we initialize the model and pytorch components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1024\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = RCF(num_features).eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c804888",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read dataset of (lat, lon) points and corresponding labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462a61e",
   "metadata": {},
   "source": [
    "We read a CSV of 100,000 randomly sampled (lat, lon) points over the US and the corresponding population living roughly within 1km$^2$ of the points from the [Gridded Population of the World](https://sedac.ciesin.columbia.edu/downloads/data/gpw-v4/gpw-v4-population-density-rev10/gpw-v4-population-density-rev10_2015_30_sec_tif.zip) dataset. This data comes from the [Code Ocean capsule](https://codeocean.com/capsule/6456296/tree/v2) that accompanies the Rolf et al. 2021 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c0affb-b0b0-46c3-8384-f8a5e7f49cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"data/unified_crop_data.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f469d400-383f-4cac-b4bd-28e1d261d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e52c4e",
   "metadata": {},
   "source": [
    "### Extract features from the imagery around each point\n",
    "\n",
    "We need to find a suitable Sentinel 2 scene for each point. As usual, we'll use `pystac-client` to search for items matching some conditions, but we don't just want to make a `.search()` call for each of the 67,968 remaining points. Each HTTP request is relatively slow. Instead, we will *batch* or points and search *in parallel*.\n",
    "\n",
    "We need to be a bit careful with how we batch up our points though. Since a single Sentinel 2 scene will cover many points, we want to make sure that points which are spatially close together end up in the same batch. In short, we need to spatially partition the dataset. This is implemented in `dask-geopandas`.\n",
    "\n",
    "So the overall workflow will be\n",
    "\n",
    "1. Find an appropriate STAC item for each point (in parallel, using the spatially partitioned dataset)\n",
    "2. Feed the points and STAC items to a custom Dataset that can read imagery given a point and the URL of a overlapping S2 scene\n",
    "3. Use a custom Dataloader, which uses our Dataset, to feed our model imagery and save the corresponding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d7aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPARTITIONS = 250\n",
    "\n",
    "ddf = dask_geopandas.from_geopandas(gdf, npartitions=1)\n",
    "hd = ddf.hilbert_distance().compute()\n",
    "gdf[\"hd\"] = hd\n",
    "gdf = gdf.sort_values(\"hd\")\n",
    "\n",
    "dgdf = dask_geopandas.from_geopandas(gdf, npartitions=NPARTITIONS, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a91d76-806d-4813-9486-24a28fe15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ae053",
   "metadata": {},
   "source": [
    "We'll write a helper function that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3bbc7f0-f3e3-413c-98e7-bc798164871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(points):\n",
    "    \"\"\"\n",
    "    Find a STAC item for points in the `points` DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : gpd.GeoDataFrame\n",
    "        A GeoDataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.GeoDataFrame\n",
    "        A new gpd.GeoDataFrame with a `stac_item` column containing the STAC\n",
    "        item that covers each point.\n",
    "    \"\"\"\n",
    "    intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
    "    \n",
    "    # https://stacspec.org/\n",
    "    search_start = \"2018-01-01\"\n",
    "    search_end = \"2019-12-31\"\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/sas/v1/sign?href={url}\"\n",
    "        \"https://planetarycomputer.microsoft.com/api/sas/v1/\"\n",
    "    )\n",
    "\n",
    "    # The time frame in which we search for non-cloudy imagery\n",
    "    search = catalog.search(\n",
    "        collections=[\"landsat-c2\"],\n",
    "        intersects=intersects,\n",
    "        datetime=[search_start, search_end],\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "        limit=500,\n",
    "    )\n",
    "    ic = search.get_all_items_as_dict()\n",
    "\n",
    "    features = ic[\"features\"]\n",
    "    features_d = {item[\"id\"]: item for item in features}\n",
    "\n",
    "    data = {\n",
    "        \"eo:cloud_cover\": [],\n",
    "        \"geometry\": [],\n",
    "    }\n",
    "\n",
    "    index = []\n",
    "\n",
    "    for item in features:\n",
    "        data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
    "        data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
    "        index.append(item[\"id\"])\n",
    "\n",
    "    items = (\n",
    "        gpd.GeoDataFrame(data, index=index, geometry=\"geometry\")\n",
    "                 .sort_values(\"eo:cloud_cover\")\n",
    "    )\n",
    "    point_list = points.geometry.tolist()\n",
    "\n",
    "    point_items = []\n",
    "    for point in point_list:\n",
    "        covered_by = items[items.covers(point)]\n",
    "        if len(covered_by):\n",
    "            point_items.append(features_d[covered_by.index[0]])\n",
    "        else:\n",
    "            # There weren't any scenes matching our conditions for this point (too cloudy)\n",
    "            point_items.append(None)\n",
    "\n",
    "    return points.assign(stac_item=point_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39f1aa-3a8e-4444-a042-945880e03efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query(points):\n",
    "#     \"\"\"\n",
    "#     Find a STAC item for points in the `points` DataFrame\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     points : gpd.GeoDataFrame\n",
    "#         A GeoDataFrame\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     gpd.GeoDataFrame\n",
    "#         A new gpd.GeoDataFrame with a `stac_item` column containing the STAC\n",
    "#         item that covers each point.\n",
    "#     \"\"\"\n",
    "#     intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
    "    \n",
    "#     # https://stacspec.org/\n",
    "#     search_start = \"2018-01-01\"\n",
    "#     search_end = \"2019-12-31\"\n",
    "#     catalog = pystac_client.Client.open(\n",
    "#         \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "#     )\n",
    "\n",
    "#     # The time frame in which we search for non-cloudy imagery\n",
    "#     search = catalog.search(\n",
    "#         collections=[\"landsat-8-c2-l2\"],\n",
    "#         intersects=intersects,\n",
    "#         datetime=[search_start, search_end],\n",
    "#         query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "#         limit=500,\n",
    "#     )\n",
    "#     ic = search.get_all_items_as_dict()\n",
    "\n",
    "#     features = ic[\"features\"]\n",
    "#     features_d = {item[\"id\"]: item for item in features}\n",
    "\n",
    "#     data = {\n",
    "#         \"eo:cloud_cover\": [],\n",
    "#         \"geometry\": [],\n",
    "#     }\n",
    "\n",
    "#     index = []\n",
    "\n",
    "#     for item in features:\n",
    "#         data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
    "#         data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
    "#         index.append(item[\"id\"])\n",
    "\n",
    "#     items = (\n",
    "#         gpd.GeoDataFrame(data, index=index, geometry=\"geometry\")\n",
    "#                  .sort_values(\"eo:cloud_cover\")\n",
    "#     )\n",
    "#     point_list = points.geometry.tolist()\n",
    "\n",
    "#     point_items = []\n",
    "#     for point in point_list:\n",
    "#         covered_by = items[items.covers(point)]\n",
    "#         if len(covered_by):\n",
    "#             point_items.append(features_d[covered_by.index[0]])\n",
    "#         else:\n",
    "#             # There weren't any scenes matching our conditions for this point (too cloudy)\n",
    "#             point_items.append(None)\n",
    "\n",
    "#     return points.assign(stac_item=point_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e55349b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query(points):\n",
    "#     \"\"\"\n",
    "#     Find a STAC item for points in the `points` DataFrame\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     points : gpd.GeoDataFrame\n",
    "#         A GeoDataFrame\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     gpd.GeoDataFrame\n",
    "#         A new gpd.GeoDataFrame with a `stac_item` column containing the STAC\n",
    "#         item that covers each point.\n",
    "#     \"\"\"\n",
    "#     intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
    "    \n",
    "#     # https://stacspec.org/\n",
    "#     search_start = \"2018-01-01\"\n",
    "#     search_end = \"2019-12-31\"\n",
    "#     catalog = pystac_client.Client.open(\n",
    "#         \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "#     )\n",
    "\n",
    "#     # The time frame in which we search for non-cloudy imagery\n",
    "#     search = catalog.search(\n",
    "#         collections=[\"landsat-8-c2-l2\"],\n",
    "#         intersects=intersects,\n",
    "#         datetime=[search_start, search_end],\n",
    "#         query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "#         limit=500,\n",
    "#     )\n",
    "#     ic = search.get_all_items_as_dict()\n",
    "\n",
    "#     features = ic[\"features\"]\n",
    "#     features_d = {item[\"id\"]: item for item in features}\n",
    "\n",
    "#     data = {\n",
    "#         \"eo:cloud_cover\": [],\n",
    "#         \"geometry\": [],\n",
    "#     }\n",
    "\n",
    "#     index = []\n",
    "\n",
    "#     for item in features:\n",
    "#         data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
    "#         data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
    "#         index.append(item[\"id\"])\n",
    "\n",
    "#     items = (\n",
    "#         gpd.GeoDataFrame(data, index=index, geometry=\"geometry\")\n",
    "#                  .sort_values(\"eo:cloud_cover\")\n",
    "#     )\n",
    "#     point_list = points.geometry.tolist()\n",
    "\n",
    "#     point_items = []\n",
    "#     for point in point_list:\n",
    "#         covered_by = items[items.covers(point)]\n",
    "#         if len(covered_by):\n",
    "#             point_items.append(features_d[covered_by.index[0]])\n",
    "#         else:\n",
    "#             # There weren't any scenes matching our conditions for this point (too cloudy)\n",
    "#             point_items.append(None)\n",
    "\n",
    "#     return points.assign(stac_item=point_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5350c952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/cullen.molitor@gmail.com/proxy/8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (          adm0  ...         hd\n",
      "1454  tanzania  ...  818840140\n",
      "1476  tanzania  ...  818984920\n",
      "1477  tanzania  ...  818984920\n",
      "698       None  ...  818984920\n",
      "826       None  ...  818984920\n",
      "1478  tanzania  ...  818984920\n",
      "1472  tanzania  ...  818984920\n",
      "1473  tanzania  ...  818984920\n",
      "1479  tanzania  ...  818984920\n",
      "1475  tanzania  ...  818984920\n",
      "1474  tanzania  ...  818984920\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (       adm0  ...         hd\n",
      "24  nigeria  ...  317971349\n",
      "35  nigeria  ...  317971349\n",
      "0   nigeria  ...  318663713\n",
      "11  nigeria  ...  318663713\n",
      "10  nigeria  ...  318663713\n",
      "9   nigeria  ...  318663713\n",
      "7   nigeria  ...  318663713\n",
      "6   nigeria  ...  318663713\n",
      "8   nigeria  ...  318663713\n",
      "4   nigeria  ...  318663713\n",
      "3   nigeria  ...  318663713\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (          adm0  ...         hd\n",
      "1481  tanzania  ...  819438078\n",
      "1482  tanzania  ...  819438078\n",
      "1484  tanzania  ...  819438078\n",
      "1485  tanzania  ...  819438078\n",
      "1486  tanzania  ...  819438078\n",
      "1487  tanzania  ...  819438078\n",
      "1488  tanzania  ...  819438078\n",
      "700       None  ...  819438078\n",
      "1483  tanzania  ...  819438078\n",
      "1489  tanzania  ...  819795758\n",
      "1490  tanzania  ...  819795758\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "2510  zambia  ...  987846368\n",
      "2383  zambia  ...  987846368\n",
      "2384  zambia  ...  987846368\n",
      "2385  zambia  ...  987846368\n",
      "1567  zambia  ...  987846368\n",
      "2386  zambia  ...  987846368\n",
      "2388  zambia  ...  987846368\n",
      "2389  zambia  ...  987846368\n",
      "2390  zambia  ...  987846368\n",
      "2396  zambia  ...  989665124\n",
      "2395  zambia  ...  989665124\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "280  nigeria  ...  348975178\n",
      "284  nigeria  ...  348975178\n",
      "282  nigeria  ...  348975178\n",
      "281  nigeria  ...  348975178\n",
      "286  nigeria  ...  348975178\n",
      "287  nigeria  ...  348975178\n",
      "283  nigeria  ...  348975178\n",
      "285  nigeria  ...  348975178\n",
      "370  nigeria  ...  350613993\n",
      "369  nigeria  ...  350613993\n",
      "368  nigeria  ...  350613993\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "226  nigeria  ...  369343458\n",
      "221  nigeria  ...  369343458\n",
      "239  nigeria  ...  372487510\n",
      "238  nigeria  ...  372487510\n",
      "236  nigeria  ...  372487510\n",
      "235  nigeria  ...  372487510\n",
      "234  nigeria  ...  372487510\n",
      "237  nigeria  ...  372487510\n",
      "232  nigeria  ...  372487510\n",
      "231  nigeria  ...  372487510\n",
      "230  nigeria  ...  372487510\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (          adm0  ...         hd\n",
      "1491  tanzania  ...  819795758\n",
      "1493  tanzania  ...  819795758\n",
      "1492  tanzania  ...  819795758\n",
      "1146      None  ...  820072126\n",
      "1389      None  ...  820072126\n",
      "1145      None  ...  820088715\n",
      "1387      None  ...  821327895\n",
      "1144      None  ...  821327895\n",
      "1388      None  ...  821521889\n",
      "1514      None  ...  821711961\n",
      "1515      None  ...  821717780\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Heartbeat to scheduler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 205, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/worker.py\", line 1260, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\n",
      "    return await coro()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 895, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 672, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 221, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55932 remote=tcp://127.0.0.1:37907>: Stream is closed\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "1959  zambia  ...  942547336\n",
      "1958  zambia  ...  942547336\n",
      "1957  zambia  ...  942547336\n",
      "1956  zambia  ...  942547336\n",
      "1955  zambia  ...  942547336\n",
      "1954  zambia  ...  942547336\n",
      "1967  zambia  ...  942547336\n",
      "1961  zambia  ...  942547336\n",
      "2247  zambia  ...  942953906\n",
      "1905  zambia  ...  942953906\n",
      "2066  zambia  ...  942953906\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Heartbeat to scheduler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 205, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/worker.py\", line 1260, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\n",
      "    return await coro()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 895, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 672, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 221, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55928 remote=tcp://127.0.0.1:37907>: Stream is closed\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...          hd\n",
      "2022  zambia  ...  1030965595\n",
      "2023  zambia  ...  1030965595\n",
      "2024  zambia  ...  1030965595\n",
      "2025  zambia  ...  1030965595\n",
      "2026  zambia  ...  1030965595\n",
      "2027  zambia  ...  1030965595\n",
      "2029  zambia  ...  1030965595\n",
      "2033  zambia  ...  1031590873\n",
      "1781  zambia  ...  1031590873\n",
      "1256    None  ...  1033993037\n",
      "1137    None  ...  1034060161\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "96   nigeria  ...  314928852\n",
      "107  nigeria  ...  314928852\n",
      "106  nigeria  ...  314928852\n",
      "105  nigeria  ...  314928852\n",
      "104  nigeria  ...  314928852\n",
      "103  nigeria  ...  314928852\n",
      "97   nigeria  ...  314928852\n",
      "101  nigeria  ...  314928852\n",
      "100  nigeria  ...  314928852\n",
      "99   nigeria  ...  314928852\n",
      "98   nigeria  ...  314928852\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...          hd\n",
      "2005  zambia  ...  1001804482\n",
      "2003  zambia  ...  1001804482\n",
      "2004  zambia  ...  1001804482\n",
      "2007  zambia  ...  1001804482\n",
      "2006  zambia  ...  1001804482\n",
      "2002  zambia  ...  1001804482\n",
      "2001  zambia  ...  1001804482\n",
      "2000  zambia  ...  1001804482\n",
      "1999  zambia  ...  1001804482\n",
      "1998  zambia  ...  1001804482\n",
      "1997  zambia  ...  1001804482\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Heartbeat to scheduler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 205, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/worker.py\", line 1260, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\n",
      "    return await coro()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 895, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 672, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 221, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55956 remote=tcp://127.0.0.1:37907>: Stream is closed\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (          adm0  ...         hd\n",
      "1375      None  ...  862949096\n",
      "1362  tanzania  ...  863112932\n",
      "1361  tanzania  ...  863112932\n",
      "1284      None  ...  863112932\n",
      "1360  tanzania  ...  863112932\n",
      "1401      None  ...  863112932\n",
      "1357  tanzania  ...  863112932\n",
      "1356  tanzania  ...  863112932\n",
      "1355  tanzania  ...  863112932\n",
      "1337      None  ...  863112932\n",
      "1359  tanzania  ...  863112932\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "1883  zambia  ...  936926977\n",
      "2198  zambia  ...  936926977\n",
      "1874  zambia  ...  936926977\n",
      "2228  zambia  ...  937301734\n",
      "2229  zambia  ...  937301734\n",
      "2225  zambia  ...  937301734\n",
      "2224  zambia  ...  937301734\n",
      "2223  zambia  ...  937301734\n",
      "2220  zambia  ...  937301734\n",
      "2221  zambia  ...  937301734\n",
      "1888  zambia  ...  937301734\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (       adm0  ...         hd\n",
      "22  nigeria  ...  418518809\n",
      "16  nigeria  ...  418518809\n",
      "23  nigeria  ...  418518809\n",
      "85  nigeria  ...  422637392\n",
      "87  nigeria  ...  422637392\n",
      "84  nigeria  ...  422637392\n",
      "95  nigeria  ...  422637392\n",
      "88  nigeria  ...  422637392\n",
      "89  nigeria  ...  422637392\n",
      "90  nigeria  ...  422637392\n",
      "91  nigeria  ...  422637392\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Heartbeat to scheduler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 205, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/worker.py\", line 1260, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\n",
      "    return await coro()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 895, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 672, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 221, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55958 remote=tcp://127.0.0.1:37907>: Stream is closed\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (          adm0  ...         hd\n",
      "1012  tanzania  ...  833943385\n",
      "1009  tanzania  ...  833943385\n",
      "967   tanzania  ...  834204369\n",
      "968   tanzania  ...  834204369\n",
      "969   tanzania  ...  834204369\n",
      "974   tanzania  ...  834204369\n",
      "970   tanzania  ...  834204369\n",
      "584       None  ...  834204369\n",
      "971   tanzania  ...  834204369\n",
      "972   tanzania  ...  834204369\n",
      "973   tanzania  ...  834204369\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "2168  zambia  ...  970557992\n",
      "2121  zambia  ...  971441931\n",
      "2152  zambia  ...  972654155\n",
      "1592  zambia  ...  972654155\n",
      "2142  zambia  ...  972654155\n",
      "2143  zambia  ...  972654155\n",
      "2144  zambia  ...  972654155\n",
      "2145  zambia  ...  972654155\n",
      "2540  zambia  ...  972654155\n",
      "2147  zambia  ...  972654155\n",
      "2148  zambia  ...  972654155\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (         adm0  ...         hd\n",
      "471  tanzania  ...  812975025\n",
      "835      None  ...  812975025\n",
      "475  tanzania  ...  812975025\n",
      "476  tanzania  ...  812975025\n",
      "478  tanzania  ...  812975025\n",
      "701      None  ...  812975025\n",
      "473  tanzania  ...  812975025\n",
      "477  tanzania  ...  812975025\n",
      "474  tanzania  ...  812975025\n",
      "736  tanzania  ...  813012497\n",
      "737  tanzania  ...  813012497\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (          adm0  ...         hd\n",
      "1100  tanzania  ...  869044975\n",
      "1099  tanzania  ...  869044975\n",
      "1098  tanzania  ...  869044975\n",
      "560       None  ...  869044975\n",
      "1321      None  ...  869044975\n",
      "1303      None  ...  869255975\n",
      "1080  tanzania  ...  869255975\n",
      "1081  tanzania  ...  869255975\n",
      "1082  tanzania  ...  869255975\n",
      "1083  tanzania  ...  869255975\n",
      "1085  tanzania  ...  869255975\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://127.0.0.1:55972 remote=tcp://127.0.0.1:37907>\n",
      "distributed.worker - WARNING - Heartbeat to scheduler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 1064, in connect\n",
      "    comm = await fut\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/worker.py\", line 1260, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\n",
      "    return await coro()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 892, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/core.py\", line 1075, in connect\n",
      "    raise CommClosedError(\n",
      "distributed.comm.core.CommClosedError: ConnectionPool not running. Status: Status.closed\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  subgraph_callable-0010cd93-a009-404b-b614-b3f2de44\n",
      "args:      (        adm0  ...         hd\n",
      "1705  zambia  ...  948596609\n",
      "1706  zambia  ...  948596609\n",
      "1707  zambia  ...  948596609\n",
      "1708  zambia  ...  948596609\n",
      "1709  zambia  ...  948596609\n",
      "1710  zambia  ...  948596609\n",
      "1711  zambia  ...  948596609\n",
      "1712  zambia  ...  948596609\n",
      "1713  zambia  ...  948596609\n",
      "1714  zambia  ...  948596609\n",
      "2137  zambia  ...  948596609\n",
      "\n",
      "[11 rows x 19 columns])\n",
      "kwargs:    {}\n",
      "Exception: 'APIError(\\'{ \"statusCode\": 404, \"message\": \"Resource not found\" }\\')'\n",
      "\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{ \"statusCode\": 404, \"message\": \"Resource not found\" }",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m                 \u001b[0mlocal_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m             return self.sync(\n\u001b[0m\u001b[1;32m   1981\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m                 \u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             return sync(\n\u001b[0m\u001b[1;32m    869\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             )\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1843\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/optimization.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected %d args, got %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/core.py\u001b[0m in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/core.py\u001b[0m in \u001b[0;36m_execute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# temporaries by their reference count and can execute certain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# operations in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/utils.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mapply_and_enforce\u001b[0;34m()\u001b[0m\n\u001b[1;32m   6010\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6011\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6012\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_dataframe_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_series_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_index_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_356/3381031823.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msearch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2018-01-01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msearch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2019-12-31\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     catalog = pystac_client.Client.open(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"https://planetarycomputer.microsoft.com/api/sas/v1/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac_client/client.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mcatalog\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mCatalog\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0msearch_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'search'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# if there is a search link, but no conformsTo advertised, ignore conformance entirely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac_client/client.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mstac_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStacApiIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstac_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stac_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conformsTo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac/catalog.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mstac_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpystac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStacIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstac_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatalog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mpystac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTACTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{result} is not a {Catalog}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac/stac_object.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_absolute_href\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstac_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmigrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac/stac_io.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mgiven\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac_client/stac_api_io.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pystac_client/stac_api_io.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_text_to_href\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: { \"statusCode\": 404, \"message\": \"Resource not found\" }"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Client(n_workers=16) as client:\n",
    "    print(client.dashboard_link)\n",
    "    meta = dgdf._meta.assign(stac_item=[])\n",
    "    df2 = dgdf.map_partitions(query, meta=meta).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eab13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.dropna(subset=[\"stac_item\"])\n",
    "\n",
    "matching_urls = [\n",
    "    [pc.sign(item[\"assets\"][\"SR_B5\"][\"href\"]) for item in df3.stac_item.tolist()],\n",
    "    [pc.sign(item[\"assets\"][\"SR_B6\"][\"href\"]) for item in df3.stac_item.tolist()],\n",
    "    [pc.sign(item[\"assets\"][\"SR_B7\"][\"href\"]) for item in df3.stac_item.tolist()]\n",
    "]\n",
    "\n",
    "points = df3[[\"lon\", \"lat\"]].to_numpy()\n",
    "population_log = np.log10(df3[\"population\"].to_numpy() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26646f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, points, fns, buffer=500):\n",
    "        self.points = points\n",
    "        self.fns = fns\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        lon, lat = self.points[idx]\n",
    "        fn = self.fns[idx]\n",
    "\n",
    "        if fn is None:\n",
    "            return None\n",
    "        else:\n",
    "            point_geom = shapely.geometry.mapping(shapely.geometry.Point(lon, lat))\n",
    "\n",
    "            with rasterio.Env():\n",
    "                with rasterio.open(fn, \"r\") as f:\n",
    "                    point_geom = rasterio.warp.transform_geom(\n",
    "                        \"epsg:4326\", f.crs.to_string(), point_geom\n",
    "                    )\n",
    "                    point_shape = shapely.geometry.shape(point_geom)\n",
    "                    mask_shape = point_shape.buffer(self.buffer).envelope\n",
    "                    mask_geom = shapely.geometry.mapping(mask_shape)\n",
    "                    try:\n",
    "                        out_image, out_transform = rasterio.mask.mask(\n",
    "                            f, [mask_geom], crop=True\n",
    "                        )\n",
    "                    except ValueError as e:\n",
    "                        if \"Input shapes do not overlap raster.\" in str(e):\n",
    "                            return None\n",
    "\n",
    "            out_image = out_image / 255.0\n",
    "            out_image = torch.from_numpy(out_image).float()\n",
    "            return out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(points, matching_urls)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count() * 2,\n",
    "    collate_fn=lambda x: x,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.zeros((points.shape[0], num_features), dtype=float)\n",
    "\n",
    "tic = time.time()\n",
    "i = 0\n",
    "for images in dataloader:\n",
    "    for image in images:\n",
    "\n",
    "        if image is not None:\n",
    "            # A full image should be ~101x101 pixels (i.e. ~1km^2 at a 10m/px spatial\n",
    "            # resolution), however we can receive smaller images if an input point\n",
    "            # happens to be at the edge of a S2 scene (a literal edge case). To deal\n",
    "            # with these (edge) cases we crudely drop all images where the spatial\n",
    "            # dimensions aren't both greater than 20 pixels.\n",
    "            if image.shape[1] >= 20 and image.shape[2] >= 20:\n",
    "                image = image.to(device)\n",
    "                with torch.no_grad():\n",
    "                    feats = model(image.unsqueeze(0)).cpu().numpy()\n",
    "\n",
    "                x_all[i] = feats\n",
    "            else:\n",
    "                # this happens if the point is close to the edge of a scene\n",
    "                # (one or both of the spatial dimensions of the image are very small)\n",
    "                pass\n",
    "        else:\n",
    "            pass  # this happens if we do not find a S2 scene for some point\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\n",
    "                f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
    "                + f\" -- {time.time()-tic:0.2f} seconds\"\n",
    "            )\n",
    "            tic = time.time()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cce22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the extracted features and given labels to model population density as a function of imagery\n",
    "\n",
    "We split the available data 80/20 into train/test. We use a cross-validation approach to tune the regularization parameter of a Ridge regression model, then apply the model to the test data and measure the R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = population_log.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a3dd6",
   "metadata": {},
   "source": [
    "And one final masking -- any sample that has all zeros for features means that we were unsuccessful at extracting features for that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nofeature_mask = ~(x_all.sum(axis=1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = x_all[nofeature_mask]\n",
    "y_all = y_all[nofeature_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8520e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_random = RidgeCV(cv=5, alphas=np.logspace(-8, 8, base=10, num=17))\n",
    "ridge_cv_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f417356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation R2 performance {ridge_cv_random.best_score_:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8acfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.maximum(ridge_cv_random.predict(x_test), 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, y_test, alpha=0.2, s=4)\n",
    "plt.xlabel(\"Predicted\", fontsize=15)\n",
    "plt.ylabel(\"Ground Truth\", fontsize=15)\n",
    "plt.title(r\"$\\log_{10}(1 + $people$/$km$^2)$\", fontsize=15)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlim([0, 6])\n",
    "plt.ylim([0, 6])\n",
    "\n",
    "plt.text(\n",
    "    0.5,\n",
    "    5,\n",
    "    s=\"R$^2$ = %0.2f\" % (r2_score(y_test, y_pred)),\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "m, b = np.polyfit(y_pred, y_test, 1)\n",
    "plt.plot(y_pred, m * y_pred + b, color=\"black\")\n",
    "plt.gca().spines.right.set_visible(False)\n",
    "plt.gca().spines.top.set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17f81e",
   "metadata": {},
   "source": [
    "In addition to a R$^2$ value of ~0.55 on the test points, we can see that we have a rank-order correlation (spearman's r) of 0.66."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23655d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495f7b6",
   "metadata": {},
   "source": [
    "### Spatial extrapolation\n",
    "\n",
    "In the previous section we split the points randomly and found that our model can _interpolate_ population density with an R2 of 0.55, however this result does not say anything about how well the model will extrapolate. Whenever you are modeling spatio-temporal data it is important to consider what the model is doing as well as the purpose of the model, then evaluate it appropriately. Here, we test how our modeling approach above is able to extrapolate to areas that it has not been trained on. Specifically we train the linear model with data from the _western_ portion of the US then test it on data from the _eastern_ US and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = points[nofeature_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c8a6e",
   "metadata": {},
   "source": [
    "First we calculate the 80th percentile longitude of the points in our dataset. Points that are to the west of this value will be in our training split and points to the east of this will be in our testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lon = np.percentile(points[:, 0], 80)\n",
    "train_idxs = np.where(points[:, 0] <= split_lon)[0]\n",
    "test_idxs = np.where(points[:, 0] > split_lon)[0]\n",
    "\n",
    "x_train = x_all[train_idxs]\n",
    "x_test = x_all[test_idxs]\n",
    "y_train = y_all[train_idxs]\n",
    "y_test = y_all[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac0a57",
   "metadata": {},
   "source": [
    "Visually, the split looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(points[:, 0], points[:, 1], c=y_all, s=1)\n",
    "plt.vlines(\n",
    "    split_lon,\n",
    "    ymin=points[:, 1].min(),\n",
    "    ymax=points[:, 1].max(),\n",
    "    color=\"black\",\n",
    "    linewidth=4,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV(cv=5, alphas=np.logspace(-8, 8, base=10, num=17))\n",
    "ridge_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd5663",
   "metadata": {},
   "source": [
    "We can see that our validation performance is similar to that of the random split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation R2 performance {ridge_cv.best_score_:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe52f0",
   "metadata": {},
   "source": [
    "However, our _test_ R$^2$ is much lower, 0.13 compared to 0.55. This shows that the linear model trained on **MOSAIKS** features and population data sampled from the _western_ US is not able to predict the population density in the _eastern_ US as well. However, from the scatter plot we can see that the predictions aren't random which warrants further investigation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.maximum(ridge_cv.predict(x_test), 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, y_test, alpha=0.2, s=4)\n",
    "plt.xlabel(\"Predicted\", fontsize=15)\n",
    "plt.ylabel(\"Ground Truth\", fontsize=15)\n",
    "plt.title(r\"$\\log_{10}(1 + $people$/$km$^2)$\", fontsize=15)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlim([0, 6])\n",
    "plt.ylim([0, 6])\n",
    "\n",
    "plt.text(\n",
    "    0.5,\n",
    "    5,\n",
    "    s=\"R$^2$ = %0.2f\" % (r2_score(y_test, y_pred)),\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "m, b = np.polyfit(y_pred, y_test, 1)\n",
    "plt.plot(y_pred, m * y_pred + b, color=\"black\")\n",
    "plt.gca().spines.right.set_visible(False)\n",
    "plt.gca().spines.top.set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430151e7",
   "metadata": {},
   "source": [
    "The rank-order correlation is still high, 0.61 compared to 0.66 from the random split. This shows that the model is still able to correctly _order_ the density of input points, however is wrong about the magnitude of the population densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e2e0b",
   "metadata": {},
   "source": [
    "This makes sense when we compare the distributions of population density of points from the western US to that of the eastern US -- the label distributions are completely different. The distribution of **MOSAIKS** features likely doesn't change, however the mapping between these features and population density definitely varies with space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc15f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 5, num=50)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y_train, bins=bins)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(r\"$\\log_{10}(1 + $people$/$km$^2)$\")\n",
    "plt.title(\"Train points -- western US\")\n",
    "plt.gca().spines.right.set_visible(False)\n",
    "plt.gca().spines.top.set_visible(False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y_test, bins=bins)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(r\"$\\log_{10}(1 + $people$/$km$^2)$\")\n",
    "plt.title(\"Test points -- eastern US\")\n",
    "plt.gca().spines.right.set_visible(False)\n",
    "plt.gca().spines.top.set_visible(False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67397753",
   "metadata": {},
   "source": [
    "Estimating the _relative_ population density of points from the corresponding imagery is still useful in a wide variety of applications, e.g. in disaster reponse settings it might make sense to allocate the most resources to the most densely populated locations, where the precise density isn't as important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
